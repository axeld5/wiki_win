{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "openai_key = config[\"openai_key\"]\n",
    "anthropic_key = config[\"anthropic_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
    "\n",
    "chat = ChatAnthropic(temperature=0, model_name=\"claude-3-haiku-20240307\")\n",
    "system = (\n",
    "    \"\"\"You are an AI language model tasked to beat the Wiki Game. \n",
    "    Your goal is, from a wikipedia page, to get the next link to click to go to the next page.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HÃ¸rring cabinet']\n"
     ]
    }
   ],
   "source": [
    "from wikipedia_functions import get_random_page, get_page_content\n",
    "\n",
    "start_page = get_random_page(1)\n",
    "final_page = \"United States\"\n",
    "print(start_page)\n",
    "final_page_content = get_page_content(final_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import get_crawler_template\n",
    "\n",
    "crawler_template = get_crawler_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'revisions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwikipedia_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_page_content\n\u001b[1;32m----> 3\u001b[0m start_links \u001b[38;5;241m=\u001b[39m \u001b[43mget_page_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_page\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\axeld\\Desktop\\open_sourced\\wikipedia_run\\wiki_win\\src\\wikipedia_functions.py:15\u001b[0m, in \u001b[0;36mget_page_content\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m---> 15\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrevisions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[(.*?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, document)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'revisions'"
     ]
    }
   ],
   "source": [
    "from wikipedia_functions import get_page_content\n",
    "\n",
    "start_links = get_page_content(start_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "The goal page is \"United States\", and the available links are not directly related to the United States. The links are about Fiji, Bau Island, and Tailevu Province, which are not relevant to reaching the goal page. Since there are no links that could be used to get closer to the goal page, the best option is to choose a random link.\n",
      "</reasoning>\n",
      "\n",
      "<output>Random</output>\n",
      "Bau Island\n",
      "<reasoning>The link 'Bau (island)' is the only one available, and it does not seem to be directly related to the goal page 'United States'. Therefore, I will output \"Random\" as the link choice.</reasoning>\n",
      "<output>Random</output>\n",
      "Bau (island)\n",
      "<reasoning>\n",
      "The goal page is \"United States\", and the available links do not seem to be directly related to that topic. The links cover a wide range of topics, from Fijian language to Christianity, cannibalism, and various islands and regions in Fiji. None of these links appear to be closely related to the United States. Therefore, I will output \"Random\" as the link choice, as I cannot find a link that could be related to the goal page.\n",
      "</reasoning>\n",
      "\n",
      "<output>Random</output>\n",
      "title\n",
      "<reasoning>\n",
      "The goal page is \"United States\", and the list of links provided does not contain any direct links to that page. However, some of the links are related to the United States, such as \"United States Congress\", \"United States Armed Forces\", and \"United States Department of State\". These links could potentially lead to information about the United States, but they are not the most direct path to the goal page. Since there is no clear link that could directly lead to the goal page, the best option is to choose a random link.\n",
      "</reasoning>\n",
      "\n",
      "<output>Random</output>\n",
      "Associate of Science\n",
      "<reasoning>The link 'Associate degree#United States' seems to be the most relevant to get to the goal page 'United States'. It provides information about associate degrees in the United States, which could be a starting point to learn more about the country.</reasoning>\n",
      "<output>Associate degree#United States</output>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(current_page)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_page \u001b[38;5;241m!=\u001b[39m final_page:\n\u001b[1;32m---> 27\u001b[0m     current_links \u001b[38;5;241m=\u001b[39m \u001b[43mget_page_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_page\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage reached in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m iterations!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\axeld\\Desktop\\open_sourced\\wikipedia_run\\wiki_win\\src\\wikipedia_functions.py:15\u001b[0m, in \u001b[0;36mget_page_content\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m---> 15\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevisions\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[(.*?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, document)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import random\n",
    "\n",
    "human = crawler_template\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "chain = prompt | chat\n",
    "current_page = start_page\n",
    "current_links = start_links\n",
    "for i in range(10):\n",
    "    text = chain.invoke(\n",
    "        {\n",
    "            \"current_page\": current_page,\n",
    "            \"current_links\": current_links,\n",
    "            \"end_page\": final_page,\n",
    "            \"end_page_content\": final_page_content\n",
    "        }\n",
    "    )\n",
    "    model_output = text.content\n",
    "    print(model_output)\n",
    "    pattern = r'<output>(.*?)</output>'\n",
    "    matches = re.findall(pattern, model_output)\n",
    "    current_page = matches[0]\n",
    "    if current_page == \"Random\":\n",
    "        current_page = random.choice(current_links)\n",
    "        print(current_page)\n",
    "    if current_page != final_page:\n",
    "        current_links = get_page_content(current_page)\n",
    "    else:\n",
    "        print(f\"Page reached in {i} iterations!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
